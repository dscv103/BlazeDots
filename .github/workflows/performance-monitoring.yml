name: Performance Monitoring

on:
  push:
    branches: [main]
  pull_request:
  workflow_run:
    workflows: ["Nix CI", "NixOS Build (flake)"]
    types: [completed]

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  collect-metrics:
    name: Collect Performance Metrics
    runs-on: ubuntu-latest
    if: github.event_name != 'workflow_run' || github.event.workflow_run.conclusion == 'success'
    
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Setup Node.js (for processing)
        uses: actions/setup-node@v5
        with:
          node-version: "20"

      - name: Collect workflow metrics
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "::group::Performance Metrics Collection"
          
          # Create metrics directory
          mkdir -p metrics
          
          # Get recent workflow runs for analysis
          workflow_runs=$(curl -s \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -H "Accept: application/vnd.github+json" \
            "https://api.github.com/repos/${{ github.repository }}/actions/runs?per_page=50&status=completed")
          
          # Process and analyze timing data
          cat > metrics/analyze.js << 'EOF'
          const fs = require('fs');
          
          const runs = JSON.parse(process.argv[2]);
          const metrics = {
            timestamp: new Date().toISOString(),
            total_runs: runs.total_count,
            recent_runs: [],
            averages: {},
            trends: {}
          };
          
          // Analyze recent runs
          for (const run of runs.workflow_runs.slice(0, 10)) {
            const duration = new Date(run.updated_at) - new Date(run.created_at);
            const durationMinutes = Math.round(duration / 1000 / 60 * 10) / 10;
            
            metrics.recent_runs.push({
              id: run.id,
              name: run.name,
              status: run.status,
              conclusion: run.conclusion,
              duration_minutes: durationMinutes,
              created_at: run.created_at
            });
          }
          
          // Calculate averages by workflow
          const workflowStats = {};
          for (const run of metrics.recent_runs) {
            if (!workflowStats[run.name]) {
              workflowStats[run.name] = { times: [], success_rate: { total: 0, success: 0 } };
            }
            workflowStats[run.name].times.push(run.duration_minutes);
            workflowStats[run.name].success_rate.total++;
            if (run.conclusion === 'success') {
              workflowStats[run.name].success_rate.success++;
            }
          }
          
          for (const [workflow, stats] of Object.entries(workflowStats)) {
            const avgTime = stats.times.reduce((a, b) => a + b, 0) / stats.times.length;
            const successRate = (stats.success_rate.success / stats.success_rate.total) * 100;
            
            metrics.averages[workflow] = {
              average_duration_minutes: Math.round(avgTime * 10) / 10,
              success_rate_percent: Math.round(successRate * 10) / 10,
              sample_size: stats.times.length
            };
          }
          
          // Write results
          fs.writeFileSync('metrics/performance.json', JSON.stringify(metrics, null, 2));
          console.log('Performance metrics collected:');
          console.log(JSON.stringify(metrics.averages, null, 2));
          EOF
          
          node metrics/analyze.js "$workflow_runs"
          echo "::endgroup::"

      - name: Generate performance report
        run: |
          echo "::group::Performance Report"
          
          if [ -f metrics/performance.json ]; then
            echo "## 📊 CI Performance Metrics" >> performance_report.md
            echo "" >> performance_report.md
            echo "Generated on: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> performance_report.md
            echo "" >> performance_report.md
            
            # Extract key metrics using jq if available, otherwise use basic parsing
            if command -v jq >/dev/null 2>&1; then
              echo "### Workflow Performance" >> performance_report.md
              echo "" >> performance_report.md
              echo "| Workflow | Avg Duration | Success Rate | Samples |" >> performance_report.md
              echo "|----------|--------------|--------------|---------|" >> performance_report.md
              
              jq -r '.averages | to_entries[] | "| \(.key) | \(.value.average_duration_minutes)m | \(.value.success_rate_percent)% | \(.value.sample_size) |"' metrics/performance.json >> performance_report.md
              
              echo "" >> performance_report.md
              echo "### Recent Runs" >> performance_report.md
              echo "" >> performance_report.md
              
              total_runs=$(jq -r '.total_runs' metrics/performance.json)
              recent_avg=$(jq -r '[.recent_runs[].duration_minutes] | add / length | floor' metrics/performance.json)
              
              echo "- Total workflow runs: $total_runs" >> performance_report.md
              echo "- Recent average duration: ${recent_avg}m" >> performance_report.md
              echo "- Analysis based on last 10 completed runs" >> performance_report.md
            else
              echo "Performance data collected but jq not available for detailed reporting" >> performance_report.md
            fi
            
            echo "" >> performance_report.md
            echo "---" >> performance_report.md
            echo "*Metrics collected automatically by CI performance monitoring*" >> performance_report.md
            
            cat performance_report.md
          else
            echo "⚠️ No performance metrics collected"
          fi
          echo "::endgroup::"

      # Store metrics as artifact for trend analysis
      - name: Upload metrics artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-metrics-${{ github.run_id }}
          path: |
            metrics/
            performance_report.md
          retention-days: 30

      # Post performance report as PR comment (only for PRs)
      - name: Comment performance metrics on PR
        if: github.event_name == 'pull_request' && github.event.action != 'closed'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('performance_report.md')) {
              const report = fs.readFileSync('performance_report.md', 'utf8');
              
              // Find existing comment to update
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });
              
              const botComment = comments.data.find(comment => 
                comment.user.type === 'Bot' && 
                comment.body.includes('CI Performance Metrics')
              );
              
              const commentBody = `${report}\n\n<sub>Updated: ${new Date().toISOString()}</sub>`;
              
              if (botComment) {
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: botComment.id,
                  body: commentBody
                });
              } else {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: commentBody
                });
              }
            }

  cache-analysis:
    name: Cache Efficiency Analysis
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Analyze cache efficiency
        run: |
          echo "::group::Cache Efficiency Analysis"
          
          # This is a placeholder for cache analysis
          # In a real implementation, this would analyze cache hit rates
          # from the actual workflow runs and provide recommendations
          
          cat > cache_report.md << 'EOF'
          ## 🗄️ Cache Efficiency Report
          
          ### Cache Strategy Analysis
          
          Current caching implementation:
          - ✅ Multi-level Nix store caching
          - ✅ Evaluation result caching
          - ✅ Host-specific build caching
          - ✅ Tool-specific caches (treefmt)
          
          ### Recommendations
          
          Based on current configuration:
          1. Cache keys are optimized for flake.lock and source changes
          2. Host-specific caching reduces rebuild overhead
          3. Tool caching improves formatter performance
          
          ### Monitoring Opportunities
          
          Future enhancements could include:
          - Real-time cache hit rate tracking
          - Cache size optimization analysis
          - Build dependency analysis
          
          ---
          *Cache analysis performed automatically*
          EOF
          
          cat cache_report.md
          echo "::endgroup::"

      - name: Upload cache analysis
        uses: actions/upload-artifact@v4
        with:
          name: cache-analysis-${{ github.run_id }}
          path: cache_report.md
          retention-days: 7